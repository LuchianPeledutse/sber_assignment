{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "ec00d50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import asyncio\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from telethon import TelegramClient\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0609ad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENVIRONMENTAL VARIABLES\n",
    "DATASET_PATH = os.getenv(\"NER_DATASET_PATH\")\n",
    "BOT_USERNAME = os.getenv(\"BOT_USERNAME\")\n",
    "API_ID = os.getenv(\"API_ID\")\n",
    "API_HASH = os.getenv(\"API_HASH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9f75cd",
   "metadata": {},
   "source": [
    "### Задание 1\n",
    "- **1.1.1** Задача с точки зрения NLP состоит в обработке и трансформации текстовых данных в нужный (обычно численный) формат для применения методов ML.\n",
    "\n",
    "- **1.2.1** Задача извлечения сущностей представляет собой распознавание частей текста (скорее всего обработанного), которые представляют собой сущности (что такое сущности определется постановкой задачи), и их классификацию по средством присваивания им метки из заранее определенного набора сущностных меток (например, человек, организация, глагол и тд)\n",
    "\n",
    "- **1.2.2** Классические методы: метод, основанный на использовании регулярных выражений (работает непосредственно с текстом). Методы основанные на классическом ML (численная обработка не связанная напрямую с нейронными сетями). К классическому ML можно отнести такие подходы как вероятностные и CRF модели.\n",
    "\n",
    "- **1.3.1** Задачу NER с помощью LLM можно решать с помощью подхода prompt-engineering'a используя разные тактики (CoT, in-context, few-shot learning и тд.); Если позволяют ресурсы можно слегка (LoRA) дообучить модель на конкретной downstream задаче (в данном случае NER)\n",
    "\n",
    "- **1.4.1** Зависит от постановки задачи. Обычно оценка задачи проводится по классическим метрикам классификации accuracy, precision, recall, f1-score. Если в задаче используются LLM то часто также считется CER схожесть одной сырой строки с другой.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0f60e6",
   "metadata": {},
   "source": [
    "### Задание 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "a5c115da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(dataset_path: str = DATASET_PATH) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads data into dataframe by folder path\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_path: str\n",
    "        Path to dataset folder (folder that occurs after unzipping sample data)\n",
    "    \"\"\"\n",
    "    # Creating list to store resulting frame rows\n",
    "    row_list = []\n",
    "    # Getting generators to iterate over paths\n",
    "    raw_docs_path_generator = os.listdir(DATASET_PATH+'raw/ru')\n",
    "    annotated_answer_path_generator = os.listdir(DATASET_PATH+'annotated/ru')\n",
    "\n",
    "    for raw_doc_path in raw_docs_path_generator:\n",
    "        for annotated_answer_path in annotated_answer_path_generator:\n",
    "            # Get ids to match docs and gold answers  \n",
    "            raw_doc_id = re.findall(r\"[0-9]+\", raw_doc_path)[0]\n",
    "            annotated_answer_id = re.findall(r\"[0-9]+\", annotated_answer_path)[0]\n",
    "\n",
    "            if raw_doc_id == annotated_answer_id:\n",
    "                # Read document path to get raw text\n",
    "                with open(DATASET_PATH+'raw/ru/'+raw_doc_path, 'r') as file:\n",
    "                    raw_data_content = file.read()\n",
    "                # Read golden answer lines to get entities and ground truths\n",
    "                with open(DATASET_PATH+'annotated/ru/'+annotated_answer_path, 'r') as file:\n",
    "                    annotated_answer_lines = file.readlines()\n",
    "                \n",
    "                # Normalize answer spaces and capitalization\n",
    "                # Starting from second line to avoid ID line\n",
    "                # Transform list of lines to a list of word lists\n",
    "                splitted_answer_lines = [re.split(r\"\\s+\",line.lower()) for line in annotated_answer_lines[1:]]\n",
    "                empty_removed_answer_lines = [[word for word in line if word != ''] for line in splitted_answer_lines] \n",
    "                # Separate with tabs and spaces\n",
    "                result_list_lines = []\n",
    "                for one_word_list_line in empty_removed_answer_lines:\n",
    "                    N = len(one_word_list_line)\n",
    "                    # Calculate where the tab should be\n",
    "                    tab_indx = (N-2)//2 \n",
    "                    # Prapare for regular space separation\n",
    "                    space_sep_list = [one_word_list_line[:tab_indx],\n",
    "                                      one_word_list_line[tab_indx:N-2]]\n",
    "                    # Prepare for tab separation\n",
    "                    tab_sep = [' '.join(word_list) for word_list in space_sep_list] + one_word_list_line[-2:]\n",
    "                    result_list_lines.append('\\t'.join(tab_sep))\n",
    "                \n",
    "                # Join lines with new lines to return normalized golden answer \n",
    "                golden_answer = '\\n'.join(result_list_lines)\n",
    "                # Find entities from golden answer to add to row\n",
    "                entity = re.findall(r\"\\s[loc|per|org|evt|pro]+\\s\", golden_answer)\n",
    "                stripped_entity = [ent.strip() for ent in entity]\n",
    "\n",
    "                # Creating and appending to row_list for further df creation\n",
    "                row_to_append = {\n",
    "                    \"document_id\": raw_doc_id,\n",
    "                    \"document_text\": raw_data_content,\n",
    "                    \"entity\": stripped_entity,\n",
    "                    \"golden_answer\": golden_answer\n",
    "                }\n",
    "                row_list.append(row_to_append)\n",
    "    return pd.DataFrame(row_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "58230b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9 entries, 0 to 8\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   document_id    9 non-null      object\n",
      " 1   document_text  9 non-null      object\n",
      " 2   entity         9 non-null      object\n",
      " 3   golden_answer  9 non-null      object\n",
      "dtypes: object(4)\n",
      "memory usage: 420.0+ bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "dataset = get_dataset()\n",
    "print(dataset.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "7241cf56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>document_text</th>\n",
       "      <th>entity</th>\n",
       "      <th>golden_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1006</td>\n",
       "      <td>ru-1006\\nru\\n2018-07-09\\nhttp://polit.ru/news/...</td>\n",
       "      <td>[evt, pro, per, per, loc, loc, per, per, per, ...</td>\n",
       "      <td>brexit\\tbrexit\\tevt\\tevt-brexit\\nfacebook\\tfac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1003</td>\n",
       "      <td>ru-1003\\nru\\n2018-07-09\\nhttps://echo.msk.ru/n...</td>\n",
       "      <td>[per, loc, loc, per, per, org, per, org, per, ...</td>\n",
       "      <td>борис джонсон\\tборис джонсон\\tper\\tper-boris-j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1017</td>\n",
       "      <td>ru-1017\\nru\\n2018-07-09\\nhttp://www.unn.com.ua...</td>\n",
       "      <td>[evt, pro, per, per, per, per, loc, per, per, ...</td>\n",
       "      <td>brexit\\tbrexit\\tevt\\tevt-brexit\\nthe guardian\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  document_id                                      document_text  \\\n",
       "0        1006  ru-1006\\nru\\n2018-07-09\\nhttp://polit.ru/news/...   \n",
       "1        1003  ru-1003\\nru\\n2018-07-09\\nhttps://echo.msk.ru/n...   \n",
       "2        1017  ru-1017\\nru\\n2018-07-09\\nhttp://www.unn.com.ua...   \n",
       "\n",
       "                                              entity  \\\n",
       "0  [evt, pro, per, per, loc, loc, per, per, per, ...   \n",
       "1  [per, loc, loc, per, per, org, per, org, per, ...   \n",
       "2  [evt, pro, per, per, per, per, loc, per, per, ...   \n",
       "\n",
       "                                       golden_answer  \n",
       "0  brexit\\tbrexit\\tevt\\tevt-brexit\\nfacebook\\tfac...  \n",
       "1  борис джонсон\\tборис джонсон\\tper\\tper-boris-j...  \n",
       "2  brexit\\tbrexit\\tevt\\tevt-brexit\\nthe guardian\\...  "
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04d8684",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
